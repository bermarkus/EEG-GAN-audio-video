{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPU_Training-Alias-Free_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuroidss/EEG-GAN-audio-video/blob/main/GPU_Training_Alias_Free_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iooMpU0wSq1v"
      },
      "source": [
        "# GPU Training - Alias-Free GAN\n",
        "by duskvirkus\n",
        "\n",
        "This is a notebook for training Alias-Free GAN on a Colab GPU instance.\n",
        "\n",
        "Repository: https://github.com/duskvirkus/alias-free-gan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ygaIX_TP7A"
      },
      "source": [
        "# GPU check\n",
        "\n",
        "If this fails change the runtime type in `Runtime > Change runtime type > Select GPU`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFKRHS3TTPbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3f3724-57e6-419c-b80a-91e08df82435"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-1d414d62-3efc-9e84-a3a5-4ff14511b2bc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRc3UyhPTi_6"
      },
      "source": [
        "## Connect Google Drive\n",
        "\n",
        "This notebook is designed to be used with google drive connected. If you'd like to use it without google drive you'll have to make changes.\n",
        "\n",
        "The main reason behind this is Colab sessions automaticall shut off after a number of hours (~10 for free, ~20 for pro, ~24 pro+). This risks loosing training progress if it's not saved to persistent storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t1M2VB4Tif6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ade3da6-ef9d-4316-be36-b61fb8e99f2b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPv1ThsOU-Op"
      },
      "source": [
        "## Clone / cd into Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NBaGNEbSqPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f5ac72-2571-486d-8b79-2360062fda20"
      },
      "source": [
        "import os\n",
        "drive_path = '/content/drive/MyDrive/'\n",
        "repo_container_dir = 'colab-alias-free-gan'\n",
        "repo_name = 'alias-free-gan'\n",
        "git_repo = 'https://github.com/duskvirkus/alias-free-gan.git'\n",
        "branch_name = 'stable'\n",
        "\n",
        "working_dir = os.path.join(drive_path, repo_container_dir, repo_name)\n",
        "\n",
        "if os.path.isdir(working_dir):\n",
        "  %cd {working_dir}\n",
        "else:\n",
        "  container_path = os.path.join(drive_path, repo_container_dir)\n",
        "  os.makedirs(container_path)\n",
        "  %cd {container_path}\n",
        "  !git clone --branch {branch_name} {git_repo}\n",
        "  %cd {repo_name}\n",
        "  !mkdir pretrained"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/colab-alias-free-gan/alias-free-gan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6qSNzMhXQta"
      },
      "source": [
        "## Install Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZYccZIHSpNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f4d04fb-4c45-4254-83bc-7ab6ef2f15e4"
      },
      "source": [
        "!python install.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.4.8-py3-none-any.whl (924 kB)\n",
            "\u001b[K     |████████████████████████████████| 924 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning-bolts\n",
            "  Downloading pytorch_lightning_bolts-0.3.2-py3-none-any.whl (253 kB)\n",
            "\u001b[K     |████████████████████████████████| 253 kB 47.6 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.12.2-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 31.7 MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.10.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 46.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Collecting pydantic\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 33.5 MB/s \n",
            "\u001b[?25hCollecting pyhocon\n",
            "  Downloading pyhocon-0.3.58.tar.gz (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 47.6 MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 49 kB/s \n",
            "\u001b[?25hCollecting opensimplex\n",
            "  Downloading opensimplex-0.3-py3-none-any.whl (15 kB)\n",
            "Collecting torchmetrics>=0.4.0\n",
            "  Downloading torchmetrics-0.5.1-py3-none-any.whl (282 kB)\n",
            "\u001b[K     |████████████████████████████████| 282 kB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.7.4.3)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.62.2)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu102)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 42.6 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 43.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.6.0)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 40.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.40.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.4.1-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 46.5 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (21.2.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (8.9.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.4.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 46.2 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 50.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.5.0)\n",
            "Building wheels for collected packages: future, subprocess32, pyhocon, pathtools\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=bdbaa96c431bf0fca0ca27d19fe21153e09a28edc1b2b274ec6e3855b1d57631\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=4f4d15190bb50438c9dd70b497a2ea7f5379e0210e91e33878f71258cb0800b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pyhocon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyhocon: filename=pyhocon-0.3.58-py3-none-any.whl size=19890 sha256=27c40e5bdd85041daec615ebda6916478a4cf5d29d5ebb2ae9e101069f958b16\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/20/f9/ff360765ce6f9fc078d6599c10a8f36496e5b5011a29df1ae3\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=edc9bbd976a4cac151093c673edf39ee946344016e4c5e21feaf1cecf9c582f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built future subprocess32 pyhocon pathtools\n",
            "Installing collected packages: multidict, yarl, async-timeout, smmap, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, gitdb, future, yaspin, subprocess32, shortuuid, sentry-sdk, pytorch-lightning, pathtools, GitPython, docker-pycreds, configparser, wandb, pytorch-lightning-bolts, pyhocon, pydantic, opensimplex, opencv-python-headless, ninja\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed GitPython-3.1.24 PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 configparser-5.0.2 docker-pycreds-0.4.0 fsspec-2021.9.0 future-0.18.2 gitdb-4.0.7 multidict-5.1.0 ninja-1.10.2 opencv-python-headless-4.5.3.56 opensimplex-0.3 pathtools-0.1.2 pyDeprecate-0.3.1 pydantic-1.8.2 pyhocon-0.3.58 pytorch-lightning-1.4.8 pytorch-lightning-bolts-0.3.2 sentry-sdk-1.4.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 torchmetrics-0.5.1 wandb-0.12.2 yarl-1.6.3 yaspin-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdzlUBR5XanA"
      },
      "source": [
        "## Convert Dataset\n",
        "\n",
        "You can skip this section if you already have a dataset in the correct format.\n",
        "\n",
        "Currently only supports datasets with only one of the following dimensions of images. 256 by 256 **or** 512 by 512 **or** 1024 by 1024\n",
        "\n",
        "Preparing your dataset for conversion. Tools to prep a data set are beyond the scope of this notebook dvschultz/dataset-tools(https://github.com/dvschultz/dataset-tools) is suggested to help with this process.\n",
        "\n",
        "Structure of your dataset:\n",
        "```\n",
        "dataset_root_dir # name of your dataset is suggested\n",
        "  |- sub_directory # anything (this has to do with labels which is an unsupported feature at current time)\n",
        "    |- image01.png\n",
        "    |- images_can_have_any_names.png\n",
        "    |- they_also_be.jpg\n",
        "    |...continued # Suggested minimum size is 1000+ images.\n",
        "```\n",
        "\n",
        "The above example would result in an input of `unconverted_dataset='path/to/dataset_root_dir'`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDivrS4rgVcT"
      },
      "source": [
        "model_size = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJGmPHFM-Dzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab276ac7-b976-46c2-914e-25b96623fb02"
      },
      "source": [
        "%rmdir /content/dataset-creation\n",
        "%mkdir /content/dataset-creation\n",
        "#%mkdir /content/dataset-creation/sq-512\n",
        "!unzip -j -o -q /content/drive/MyDrive/sq-{model_size}.zip -d /content/dataset-creation/sq-{model_size}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmdir: failed to remove '/content/dataset-creation': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqswLFEkarPc",
        "outputId": "8d00219c-d7fc-42f5-bbc3-e9a06afece0c"
      },
      "source": [
        "unconverted_dataset = '/content/dataset-creation'\n",
        "out_path = '/content/drive/MyDrive/datasets-aliasfree/sq-'+model_size\n",
        "%mkdir /content/drive/MyDrive/datasets-aliasfree\n",
        "dataset_size = model_size # one of the following 256, 512, 1024\n",
        "!python scripts/convert_dataset.py --size {dataset_size} {unconverted_dataset} {out_path}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/datasets-aliasfree’: File exists\n",
            "Make dataset of image sizes: 512\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:387: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:387: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "2193it [00:29, 73.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6Yc1QbacaId"
      },
      "source": [
        "## Info on training options\n",
        "\n",
        "Most training options work rather well out of the box. See the training section for suggested arguments.\n",
        "\n",
        "You can see a full list of training options by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuc-24U3dH_l"
      },
      "source": [
        "!python scripts/trainer.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMt7IazBas5c"
      },
      "source": [
        "## Training\n",
        "\n",
        "Results from training can be found in `results` directory.\n",
        "\n",
        "**Resume from Checkpoint**\n",
        "\n",
        "Set `--resume_from 'path/to/checkpoint.pt'`\n",
        "\n",
        "If resuming from a checkpoint that doesn't use the new kimg naming scheme use `--start_kimg_count` to set the starting count manually.\n",
        "\n",
        "**Transfer Learning Options**\n",
        "\n",
        "See repository for transfer learning options. https://github.com/duskvirkus/alias-free-gan/blob/devel/pretrained_models.json\n",
        "\n",
        "Use `--resume_from 'model_name'`. wget is used to automatically download the pretrained models.\n",
        "\n",
        "**Training from Scratch**\n",
        "\n",
        "This is not recommended as transfer learning off of any model even if it's not related to your dataset will be faster and consume less resources. Unless there is no pretrained models or you have an explicit reason use transfer learning. To train from scratch simply leave resume blank, like so `--resume_from ''`.\n",
        "\n",
        "**Augmentations**\n",
        "\n",
        "Use `--augment True` to enable augmentations with `AdaptiveAugmentation`. See help for more options.\n",
        "\n",
        "### Suggested Batch Size\n",
        "\n",
        "For colab pro gpus (16GB) here are the suggested batch sizes:\n",
        "- 256: batch size 8 recommended\n",
        "- 512: batch size 4? recommended\n",
        "- 1024: batch size 4 for (p100) or 2 for (v100)\n",
        "\n",
        "Feel free to play around to see if you can get things higher. For the best performance try to keep batch in powers of 2.\n",
        "\n",
        "### Trouble Shooting\n",
        "\n",
        "If you get a cuda out of memory error try reducing the `batch`.\n",
        "\n",
        "If you get another error please report it at https://github.com/duskvirkus/alias-free-gan/issues/new\n",
        "\n",
        "If the model makes it through the first epoch you're unlike to encounter any errors after that.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI69L2vybsPr"
      },
      "source": [
        "#model_size = 512\n",
        "#dataset_location = '/content/drive/MyDrive/datasets-aliasfree/sq-512'\n",
        "dataset_location = '/content/drive/MyDrive/sq-512.zip'\n",
        "#resume = 'rosinality-ffhq-800k'\n",
        "#resume = 'pretrained/000000020-kimg-sq-256-checkpoint.pt'\n",
        "#resume = 'results/training-000003/000000066-kimg-sq-512-checkpoint.pt'\n",
        "#resume = 'results/training-0000011/000000022-kimg-sq-512-checkpoint.pt'\n",
        "batch_size = 4\n",
        "#batch_size = 8\n",
        "augmentations = True # ada\n",
        "\n",
        "sample_frequency = 1 # in kimgs or thousands of images\n",
        "checkpoint_frequency = 1 # in kimgs or thousands of images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru3lMUNHHIGW"
      },
      "source": [
        "!python scripts/trainer.py \\\n",
        "  --gpus 1 \\\n",
        "  --max_epochs 1000000 \\\n",
        "  --accumulate_grad_batches 4 \\\n",
        "  --size {model_size} \\\n",
        "  --dataset_path {dataset_location} \\\n",
        "  --batch {batch_size} \\\n",
        "  --save_sample_every_kimgs {sample_frequency} \\\n",
        "  --save_checkpoint_every_kimgs {checkpoint_frequency} \\\n",
        "  --augment {augmentations} \\\n",
        "  --auto_scale_batch_size True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cOKAW3h6RVs"
      },
      "source": [
        "#model_size = 512\n",
        "dataset_location = '/content/drive/MyDrive/datasets-aliasfree/sq-'+model_size\n",
        "#resume = 'rosinality-ffhq-800k'\n",
        "#resume = 'pretrained/000000020-kimg-sq-256-checkpoint.pt'\n",
        "#resume = 'results/training-000009/000000011-kimg-sq-512-checkpoint.pt'\n",
        "resume = 'results/training-000011/000000022-kimg-sq-512-checkpoint.pt'\n",
        "batch_size = 4\n",
        "#batch_size = 8\n",
        "augmentations = True # ada\n",
        "\n",
        "sample_frequency = 1 # in kimgs or thousands of images\n",
        "checkpoint_frequency = 1 # in kimgs or thousands of images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avx9vyhlczji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75404af-5bc5-477a-9b59-f6b036a19795"
      },
      "source": [
        "!python scripts/trainer.py \\\n",
        "  --gpus 1 \\\n",
        "  --max_epochs 1000000 \\\n",
        "  --accumulate_grad_batches 4 \\\n",
        "  --size {model_size} \\\n",
        "  --dataset_path {dataset_location} \\\n",
        "  --resume_from {resume} \\\n",
        "  --batch {batch_size} \\\n",
        "  --save_sample_every_kimgs {sample_frequency} \\\n",
        "  --save_checkpoint_every_kimgs {checkpoint_frequency} \\\n",
        "  --augment {augmentations} \\\n",
        "  --auto_scale_batch_size True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Alias-Free GAN version: 1.1.0\n",
            "Resuming from custom checkpoint...\n",
            "Dataset path: /content/drive/MyDrive/datasets-aliasfree/sq-512\n",
            "Initialized MultiResolutionDataset dataset with 2193 images\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type          | Params\n",
            "------------------------------------------------\n",
            "0 | generator     | Generator     | 14.4 M\n",
            "1 | g_ema         | Generator     | 14.4 M\n",
            "2 | discriminator | Discriminator | 29.0 M\n",
            "------------------------------------------------\n",
            "57.9 M    Trainable params\n",
            "0         Non-trainable params\n",
            "57.9 M    Total params\n",
            "231.431   Total estimated model params size (MB)\n",
            "Training: -1it [00:00, ?it/s]\n",
            "\n",
            "Resuming from: results/training-000011/000000022-kimg-sq-512-checkpoint.pt\n",
            "\n",
            "AlignFreeGAN device: cuda:0\n",
            "\n",
            "\n",
            "Epoch 0:   0% 0/548 [00:00<00:00, 3659.95it/s]  /content/drive/My Drive/colab-alias-free-gan/alias-free-gan/scripts/../src/stylegan2/op/conv2d_gradfix.py:89: UserWarning: conv2d_gradfix not supported on PyTorch 1.9.0+cu102. Falling back to torch.nn.functional.conv2d().\n",
            "  f\"conv2d_gradfix not supported on PyTorch {torch.__version__}. Falling back to torch.nn.functional.conv2d().\"\n",
            "Epoch 1:  84% 460/548 [45:09<08:37,  5.88s/it, kimgs=26.032, r_t_stat=0.875, ada_aug_p=0.005760]"
          ]
        }
      ]
    }
  ]
}