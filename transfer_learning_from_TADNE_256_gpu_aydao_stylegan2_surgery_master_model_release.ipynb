{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer learning from TADNE 256 gpu aydao-stylegan2-surgery-master-model-release.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuroidss/EEG-GAN-audio-video/blob/main/transfer_learning_from_TADNE_256_gpu_aydao_stylegan2_surgery_master_model_release.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPI5E5y0pujD"
      },
      "source": [
        "# Custom Training StyleGan2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI_i1MwgpzOD"
      },
      "source": [
        "StyleGAN2-ADA only work with Tensorflow 1. Run the next cell before anything else to make sure we’re using TF1 and not TF2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKYAU7Wub3WW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f31f6365-9e8f-4686-c6ef-5eb07db53790"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51ei6d5kxVDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77897a5b-60c2-4fe6-a81c-6d2bb5cf912c"
      },
      "source": [
        "#!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YcUMPQp6ipP"
      },
      "source": [
        "## Install Repo to Google Drive\n",
        "\n",
        "Colab is a little funky with training. I’ve found the best way to do this is to install the repo directly into your Google Drive folder.\n",
        "\n",
        "First, mount your Drive to the Colab notebook: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxxYlEKI9Gis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfdc3c04-e4a8-4e1b-a41e-871a6fca4ed5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IDcqJ2CzHXm"
      },
      "source": [
        "#!curl https://sdk.cloud.google.com | bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epV6TDzAjox1"
      },
      "source": [
        "Next, run this cell. If you’re already installed the repo, it will skip the installation process and change into the repo’s directory. If you haven’t installed it, it will install all the files necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AdOYnDszUgV"
      },
      "source": [
        "#!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HX77jscX2zV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1522fdd2-4850-484b-8731-c8aded2ff35c"
      },
      "source": [
        "import os\n",
        "if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-aydao-surgery-master\"):\n",
        "    %cd \"/content/drive/MyDrive/colab-sg2-aydao-surgery-master/stylegan2-surgery\"\n",
        "else:\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !mkdir colab-sg2-aydao-surgery-master\n",
        "    %cd colab-sg2-aydao-surgery-master\n",
        "    !git clone --branch master https://github.com/aydao/stylegan2-surgery.git\n",
        "#    !git clone --branch tpu https://github.com/shawwn/stylegan2.git\n",
        "#    !git clone https://github.com/dvschultz/stylegan2-ada\n",
        "    %cd stylegan2-surgery\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !wget -q https://raw.githubusercontent.com/aydao/stylegan2-surgery/model-release/training/networks_stylegan2.py -O ./training/networks_stylegan2.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/colab-sg2-aydao-surgery-master/stylegan2-surgery\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7oj_kBaemol"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/colab-sg2-aydao/stylegan2-surgery\"\n",
        "!git config --global user.name \"test\"\n",
        "!git config --global user.email \"test@test.com\"\n",
        "#!git fetch origin\n",
        "#!git checkout origin/main -- train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeS9tDvt61VG"
      },
      "source": [
        "## Convert dataset to .tfrecords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q58MJbckLUc"
      },
      "source": [
        "**Note: You only need to do this once per dataset. If you have already run this and are returning to conntinue training, skip these cells.**\n",
        "\n",
        "Next we need to convert our image dataset to a format that StyleGAN2-ADA can read from. There are two options here. You can upload your dataset directly to Colab (as a zipped file), or you can upload it to Drive directly and read it from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JUP51nJdEjz"
      },
      "source": [
        "#if you manually uploaded your dataset to Colab, unzip it\n",
        "zip_path = \"/content/drive/MyDrive/sq-256.zip\"\n",
        "\n",
        "!unzip {zip_path} -d /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0QH0nzjlbEE"
      },
      "source": [
        "Now that your image dataset is uploaded, we need to convert it to the `.tfrecords` format.\n",
        "\n",
        "Depending on the resolution of your images and how many you have, this can take a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-BZHhBe7AvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a38b97-d4b1-4ebc-be4f-a578dd763811"
      },
      "source": [
        "#update this to the path to your image folder\n",
        "dataset_path = \"/content/sq-256\"\n",
        "#give your dataset a name\n",
        "dataset_name = \"sq-256\"\n",
        "\n",
        "#you don't need to edit anything here\n",
        "!python dataset_tool.py create_aydao ./datasets {dataset_path} {dataset_name} 256 256\n",
        "#!python dataset_tool.py create_from_images gs://train_with_tpu/datasets/{dataset_name} {dataset_path}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from \"/content/sq-256\"\n",
            "detected 2028 images ...\n",
            "Shuffle the images... using seed 123\n",
            "Creating dataset \"./datasets\"\n",
            "Adding the images to tfrecords ...\n",
            "added images 0\n",
            "added images 1000\n",
            "added images 2000\n",
            "Added 2028 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DvTupHzP2s_"
      },
      "source": [
        "## Train a custom model\n",
        "\n",
        "We’re ready to start training! There are numerous arguments to training, what’s listed below are the most popular options. To see all the options, run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiIBpCuUmRh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2957f6-40c6-4faf-ec00-dc26ab4ed553"
      },
      "source": [
        "#!export TPU_NAME={tpu_address}\n",
        "#!export RESUME_PKL=\"/content/drive/MyDrive/sg2-ext_aydao-anime-danbooru2019s-512-5268480.pkl\"\n",
        "#!export RESUME_PKL=./results/00019-stylegan2-sq-256-1gpu-config-a/network-snapshot-000100.pkl\n",
        "#!export RESUME_KIMG=100.4\n",
        "#!export RESUME_TIME=43\n",
        "#!export RESUME_TIME=`math 10*3600 + 13*60 + 49` # 10h 13m 49s\n",
        "#RESUME_TIME=0*3600 + 44*60 + 31\n",
        "#!export RESUME_TIME={RESUME_TIME} # 10h 13m 49s\n",
        "\n",
        "#    resume_pkl=\"./results/00019-stylegan2-sq-256-1gpu-config-a/network-snapshot-000100.pkl\"\n",
        "#    resume_kimg=100.4\n",
        "#    resume_time=0*3600 + 44*60 + 31\n",
        "#    resume_pkl=\"/content/drive/MyDrive/stylegan2-cat-config-a.pkl\"\n",
        "#    resume_kimg=0\n",
        "#    resume_time=0\n",
        "\n",
        "config=\"config-f\" # StyleGAN 2\n",
        "#config=\"config-a\" # StyleGAN 1\n",
        "\n",
        "data_dir=\"./datasets\"\n",
        "#data_dir=\"/content/drive/MyDrive/colab-sg2/stylegan2/datasets\"\n",
        "dataset=\"sq-256\"\n",
        "mirror=\"False\"\n",
        "metrics=\"none\"\n",
        "#!export NO_GCE_CHECK=true\n",
        "total_kimg=5268480+1000\n",
        "#resume=\"/content/drive/MyDrive/2020-11-27-aydao-stylegan2ext-danbooru2019s-512px-5268480.pkl\"\n",
        "#resume=\"/content/drive/MyDrive/colab-sg2-aydao-surgery-master/stylegan2-surgery/results/networks/tadne-256px-5268480.pkl\"\n",
        "#resume=\"/content/drive/MyDrive/colab-sg2-aydao-surgery-master/stylegan2-surgery/results/networks/network-copyover-256px-5268480.pkl\"\n",
        "#resume=\"/content/drive/MyDrive/colab-sg2-aydao-surgery-master/stylegan2-surgery/results/00000-pretrained-networks/2020-11-27-aydao-stylegan2ext-danbooru2019s-256px-5268480.pkl\"\n",
        "#resume=\"./results/00040-stylegan2-sq-512-1gpu-config-a\"\n",
        "#resume=\"./results/00009-stylegan2-sq-256-1gpu-config-f\"\n",
        "resume=\"latest\"\n",
        "\n",
        "#!python run_training.py --height 512 --width 512 --resume-pkl \"{resume}\" --total-kimg \"{total_kimg}\" --data-dir \"{data_dir}\" --config=\"{config}\" --dataset=\"{dataset}\" --mirror-augment=\"{mirror}\" --metrics=\"{metrics}\"\n",
        "#!python run_training.py --height 512 --width 512 --total-kimg \"{total_kimg}\" --data-dir \"{data_dir}\" --config=\"{config}\" --dataset=\"{dataset}\" --mirror-augment=\"{mirror}\" --metrics=\"{metrics}\"\n",
        "!python run_training.py --cond True --height 256 --width 256 --resume-pkl \"{resume}\" --total-kimg \"{total_kimg}\" --data-dir \"{data_dir}\" --config=\"{config}\" --dataset=\"{dataset}\" --mirror-augment=\"{mirror}\" --metrics=\"{metrics}\"\n",
        "#!python run_training.py --height 256 --width 256 --resume-pkl \"{resume}\" --total-kimg \"{total_kimg}\" --data-dir \"{data_dir}\" --config=\"{config}\" --dataset=\"{dataset}\" --mirror-augment=\"{mirror}\" --metrics=\"{metrics}\"\n",
        "#!python run_training.py --cond True --height 256 --width 256 --total-kimg \"{total_kimg}\" --data-dir \"{data_dir}\" --config=\"{config}\" --dataset=\"{dataset}\" --mirror-augment=\"{mirror}\" --metrics=\"{metrics}\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local submit - run_dir: results/00001-stylegan2-sq-256-1gpu-cond-config-f\n",
            "dnnlib: Running training.training_loop.training_loop() on localhost...\n",
            "Streaming data using training.dataset.TFRecordDataset...\n",
            "Dataset shape = [3, 256, 256]\n",
            "Dynamic range = [0, 255]\n",
            "Label size    = 0\n",
            "Loading networks from \"results/00000-pretrained/network-snapshot-5268486.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Loading... Done.\n",
            "dlatent_size in gmain 1024\n",
            "dlatent_size in gmain 1024\n",
            "\n",
            "G                             Params     OutputShape         WeightShape       \n",
            "---                           ---        ---                 ---               \n",
            "latents_in                    -          (?, 1024)           -                 \n",
            "labels_in                     -          (?, 0)              -                 \n",
            "lod                           -          ()                  -                 \n",
            "dlatent_avg                   -          (1024,)             -                 \n",
            "G_mapping/latents_in          -          (?, 1024)           -                 \n",
            "G_mapping/labels_in           -          (?, 0)              -                 \n",
            "G_mapping/Dense0              1049600    (?, 1024)           (1024, 1024)      \n",
            "G_mapping/Dense1              1049600    (?, 1024)           (1024, 1024)      \n",
            "G_mapping/Dense2              1049600    (?, 1024)           (1024, 1024)      \n",
            "G_mapping/Dense3              1049600    (?, 1024)           (1024, 1024)      \n",
            "G_mapping/Broadcast           -          (?, 14, 1024)       -                 \n",
            "G_mapping/dlatents_out        -          (?, 14, 1024)       -                 \n",
            "Truncation/Lerp               -          (?, 14, 1024)       -                 \n",
            "G_synthesis/dlatents_in       -          (?, 14, 1024)       -                 \n",
            "G_synthesis/4x4/Const         16384      (?, 1024, 4, 4)     (1, 1024, 4, 4)   \n",
            "G_synthesis/4x4/Conv          10487809   (?, 1024, 4, 4)     (3, 3, 1024, 1024)\n",
            "G_synthesis/4x4/ToRGB         1052675    (?, 3, 4, 4)        (1, 1, 1024, 3)   \n",
            "G_synthesis/8x8/Conv0_up      10487809   (?, 1024, 8, 8)     (3, 3, 1024, 1024)\n",
            "G_synthesis/8x8/Conv1         10487809   (?, 1024, 8, 8)     (3, 3, 1024, 1024)\n",
            "G_synthesis/8x8/Upsample      -          (?, 3, 8, 8)        -                 \n",
            "G_synthesis/8x8/ToRGB         1052675    (?, 3, 8, 8)        (1, 1, 1024, 3)   \n",
            "G_synthesis/16x16/Conv0_up    10487809   (?, 1024, 16, 16)   (3, 3, 1024, 1024)\n",
            "G_synthesis/16x16/Conv1       10487809   (?, 1024, 16, 16)   (3, 3, 1024, 1024)\n",
            "G_synthesis/16x16/Upsample    -          (?, 3, 16, 16)      -                 \n",
            "G_synthesis/16x16/ToRGB       1052675    (?, 3, 16, 16)      (1, 1, 1024, 3)   \n",
            "G_synthesis/32x32/Conv0_up    10487809   (?, 1024, 32, 32)   (3, 3, 1024, 1024)\n",
            "G_synthesis/32x32/Conv1       10487809   (?, 1024, 32, 32)   (3, 3, 1024, 1024)\n",
            "G_synthesis/32x32/Upsample    -          (?, 3, 32, 32)      -                 \n",
            "G_synthesis/32x32/ToRGB       1052675    (?, 3, 32, 32)      (1, 1, 1024, 3)   \n",
            "G_synthesis/64x64/Conv0_up    10487809   (?, 1024, 64, 64)   (3, 3, 1024, 1024)\n",
            "G_synthesis/64x64/Conv1       10487809   (?, 1024, 64, 64)   (3, 3, 1024, 1024)\n",
            "G_synthesis/64x64/Upsample    -          (?, 3, 64, 64)      -                 \n",
            "G_synthesis/64x64/ToRGB       1052675    (?, 3, 64, 64)      (1, 1, 1024, 3)   \n",
            "G_synthesis/128x128/Conv0_up  5768705    (?, 512, 128, 128)  (3, 3, 1024, 512) \n",
            "G_synthesis/128x128/Conv1     2884609    (?, 512, 128, 128)  (3, 3, 512, 512)  \n",
            "G_synthesis/128x128/Upsample  -          (?, 3, 128, 128)    -                 \n",
            "G_synthesis/128x128/ToRGB     526339     (?, 3, 128, 128)    (1, 1, 512, 3)    \n",
            "G_synthesis/256x256/Conv0_up  1704705    (?, 256, 256, 256)  (3, 3, 512, 256)  \n",
            "G_synthesis/256x256/Conv1     852481     (?, 256, 256, 256)  (3, 3, 256, 256)  \n",
            "G_synthesis/256x256/Upsample  -          (?, 3, 256, 256)    -                 \n",
            "G_synthesis/256x256/ToRGB     263171     (?, 3, 256, 256)    (1, 1, 256, 3)    \n",
            "G_synthesis/images_out        -          (?, 3, 256, 256)    -                 \n",
            "G_synthesis/noise0            -          (1, 1, 4, 4)        -                 \n",
            "G_synthesis/noise1            -          (1, 1, 8, 8)        -                 \n",
            "G_synthesis/noise2            -          (1, 1, 8, 8)        -                 \n",
            "G_synthesis/noise3            -          (1, 1, 16, 16)      -                 \n",
            "G_synthesis/noise4            -          (1, 1, 16, 16)      -                 \n",
            "G_synthesis/noise5            -          (1, 1, 32, 32)      -                 \n",
            "G_synthesis/noise6            -          (1, 1, 32, 32)      -                 \n",
            "G_synthesis/noise7            -          (1, 1, 64, 64)      -                 \n",
            "G_synthesis/noise8            -          (1, 1, 64, 64)      -                 \n",
            "G_synthesis/noise9            -          (1, 1, 128, 128)    -                 \n",
            "G_synthesis/noise10           -          (1, 1, 128, 128)    -                 \n",
            "G_synthesis/noise11           -          (1, 1, 256, 256)    -                 \n",
            "G_synthesis/noise12           -          (1, 1, 256, 256)    -                 \n",
            "images_out                    -          (?, 3, 256, 256)    -                 \n",
            "---                           ---        ---                 ---               \n",
            "Total                         115868450                                        \n",
            "\n",
            "\n",
            "D                    Params    OutputShape       WeightShape     \n",
            "---                  ---       ---               ---             \n",
            "images_in            -         (?, 3, 256, 256)  -               \n",
            "labels_in            -         (?, 0)            -               \n",
            "256x256/FromRGB      512       (128,)            (1, 1, 3, 128)  \n",
            "256x256/Conv0        147584    (128,)            (3, 3, 128, 128)\n",
            "256x256/Conv1_down   295168    (256,)            (3, 3, 128, 256)\n",
            "256x256/Skip         32768     (1, 1, 128, 256)  (1, 1, 128, 256)\n",
            "128x128/Conv0        590080    (256,)            (3, 3, 256, 256)\n",
            "128x128/Conv1_down   1180160   (512,)            (3, 3, 256, 512)\n",
            "128x128/Skip         131072    (1, 1, 256, 512)  (1, 1, 256, 512)\n",
            "64x64/Conv0          2359808   (512,)            (3, 3, 512, 512)\n",
            "64x64/Conv1_down     2359808   (512,)            (3, 3, 512, 512)\n",
            "64x64/Skip           262144    (1, 1, 512, 512)  (1, 1, 512, 512)\n",
            "32x32/Conv0          2359808   (512,)            (3, 3, 512, 512)\n",
            "32x32/Conv1_down     2359808   (512,)            (3, 3, 512, 512)\n",
            "32x32/Skip           262144    (1, 1, 512, 512)  (1, 1, 512, 512)\n",
            "16x16/Conv0          2359808   (512,)            (3, 3, 512, 512)\n",
            "16x16/Conv1_down     2359808   (512,)            (3, 3, 512, 512)\n",
            "16x16/Skip           262144    (1, 1, 512, 512)  (1, 1, 512, 512)\n",
            "8x8/Conv0            2359808   (512,)            (3, 3, 512, 512)\n",
            "8x8/Conv1_down       2359808   (512,)            (3, 3, 512, 512)\n",
            "8x8/Skip             262144    (1, 1, 512, 512)  (1, 1, 512, 512)\n",
            "4x4/MinibatchStddev  -         (?, 516, 4, 4)    -               \n",
            "4x4/Conv             2378240   (512,)            (3, 3, 516, 512)\n",
            "4x4/Dense0           4194816   (512,)            (8192, 512)     \n",
            "Output               513       (1,)              (512, 1)        \n",
            "scores_out           -         (?, 1)            -               \n",
            "---                  ---       ---               ---             \n",
            "Total                28877953                                    \n",
            "\n",
            "dlatent_size in gmain 1024\n",
            "Building TensorFlow graph...\n",
            "dlatent_size in gmain 1024\n",
            "dlatent_size in gmain 1024\n",
            "dlatent_size in gmain 1024\n",
            "Initializing logs...\n",
            "Training for 5269480 kimg...\n",
            "\n",
            "tick 0     kimg 5268486.0 lod 0.00  minibatch 2    time 46s          sec/tick 45.6    sec/kimg 5697.22 maintenance 0.0    gpumem 9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lohotw1FqC54"
      },
      "source": [
        "### While it’s training...\n",
        "**Once the above cell is running you should be training!**\n",
        "\n",
        "Don’t close this tab! Colab needs to be open and running in order to continue training. Every ~15min or so a new line should get added to your output, indicated its still training. Depending on you `snapshot_count` setting you should see the results folder in your Google drive folder fill with both samples (`fakesXXXXXx.jpg`) and model weights (`network-snapshot-XXXXXX.pkl`). The samples are worth looking at while it trains but don’t get too worried about each individual sample.\n",
        "\n",
        "If you chose a metric, you will also see scores for each snapshot. Don’t obsess over these! they are a guide, it can go up or down slightly for each snapshot. What you want to see is a gradual lowering of the score over time.\n",
        "\n",
        "Once Colab shuts off, you can Reconnect the notebook and re-run every cell from top to bottom. Make sure you update the `resume_from` path to continue training from the latest model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZR4ireZE6JF"
      },
      "source": [
        "!python create_initial_network_pkl.py --config config-f --height 256 --width 256 --cond False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkDVmUF2QerL"
      },
      "source": [
        "\n",
        "!python copy_weights_portable.py \"/content/drive/MyDrive/2020-11-27-aydao-stylegan2ext-danbooru2019s-512px-5268480.pkl\" ./results/00005-stylegan2-sq-256-1gpu-config-f/network-snapshot-000000.pkl\n",
        "#!python copy_weights_portable.py \"/content/drive/MyDrive/2020-11-27-aydao-stylegan2ext-danbooru2019s-512px-5268480.pkl\" ./results/networks/network-initial-config-f-256x256-0.pkl\n",
        "#!python copy_weights.py \"/content/drive/MyDrive/2020-11-27-aydao-stylegan2ext-danbooru2019s-512px-5268480.pkl\" ./results/networks/network-initial-config-f-256x256-0.pkl\n",
        "#!python copy_crop_weights.py \"/content/drive/MyDrive/2020-11-27-aydao-stylegan2ext-danbooru2019s-512px-5268480.pkl\" network-initial-config-f-256x256-0.pkl\n",
        "#!python copy_crop_weights.py \"/content/drive/MyDrive/2020-11-27-aydao-stylegan2ext-danbooru2019s-512px-5268480.pkl\" network-initial-config-f-512x512-0.pkl\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyN3mtoGQgSj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}