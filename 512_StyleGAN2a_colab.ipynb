{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "512 StyleGAN2a_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuroidss/EEG-GAN-audio-video/blob/main/512_StyleGAN2a_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzA1-mt88AO_"
      },
      "source": [
        "# StyleGAN2-ada operations\n",
        "\n",
        "### Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IFfx8GQIAQm"
      },
      "source": [
        "\n",
        "**Run this cell after each session restart**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkbcraCUaPEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2408c8d8-a584-4d14-f7ad-4609d0f5393e"
      },
      "source": [
        "#@title General setup { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "!pip install torch==1.7.1 torchvision==0.8.2\n",
        "\n",
        "from IPython.display import HTML, Image, display\n",
        "from moviepy.editor import ImageSequenceClip, ipython_display\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "!apt-get -qq install ffmpeg\n",
        "!pip install ninja\n",
        "from google.colab import drive\n",
        "drive.mount('/G', force_remount=True)\n",
        "gdir = '/G/MyDrive/'\n",
        "%cd $gdir\n",
        "\n",
        "#@markdown Copying StyleGAN2 to the directory below on your Google drive (creating it, if it doesn't exist):\n",
        "work_dir = 'sg2a_eps' #@param {type:\"string\"}\n",
        "#@markdown NB: All paths below are relative to this directory (except the archive with source images on the next step). \n",
        "\n",
        "#@markdown NB: Avoid connecting Google drive manually via the icon in Files section on the left. Doing so may break further operations.\n",
        "\n",
        "work_dir = gdir + work_dir + '/'\n",
        "if not os.path.isdir(work_dir):\n",
        "  !git clone git://github.com/eps696/stylegan2ada $work_dir\n",
        "%cd $work_dir\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "from src.util.utilgan import file_list, img_list, basename\n",
        "model = model_pkl = ''\n",
        "def model_select(work_dir):\n",
        "  models = file_list(work_dir, 'pkl', subdir=True)\n",
        "  global model, model_pkl\n",
        "  model_pkl = models[0]\n",
        "  model = model_pkl.replace('.pkl', '')\n",
        "  models = [m.replace(work_dir, '') for m in models if not basename(m) in ['submit_config', 'vgg16_zhang_perceptual']]\n",
        "  def on_change(change):\n",
        "    global model, model_pkl\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "      model = change['new']\n",
        "      model = os.path.splitext(model)[0] # filename without extension => load custom network\n",
        "      model_pkl = model + '.pkl' # filename with extension => load original network\n",
        "  model_select = widgets.Dropdown(options=models, description='Found models:', style={'description_width': 'initial'}, layout={'width': 'max-content'})\n",
        "  display(model_select)\n",
        "  model_select.observe(on_change)\n",
        "# model_select(work_dir)\n",
        "\n",
        "# Hardware check\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil \n",
        "import GPUtil as GPU\n",
        "gpu = GPU.getGPUs()[0]\n",
        "!nvidia-smi -L\n",
        "print(\"GPU RAM {0:.0f}MB | Free {1:.0f}MB)\".format(gpu.memoryTotal, gpu.memoryFree))\n",
        "print('\\nDone!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 18 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2\n",
            "  Downloading torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 24 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1 torchvision-0.8.2\n",
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2433024/45929032 bytes (5.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5726208/45929032 bytes (12.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9256960/45929032 bytes (20.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12730368/45929032 bytes (27.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16097280/45929032 bytes (35.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19398656/45929032 bytes (42.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22691840/45929032 bytes (49.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25862144/45929032 bytes (56.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29106176/45929032 bytes (63.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32497664/45929032 bytes (70.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35667968/45929032 bytes (77.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38928384/45929032 bytes (84.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42237952/45929032 bytes (92.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45703168/45929032 bytes (99.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.10.2\n",
            "Mounted at /G\n",
            "/G/MyDrive\n",
            "/G/MyDrive/sg2a_eps\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (5.4.8)\n",
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=76c3e6dffb2d3e32149d1dc5e7a32977dfeeccba07bcb415316241f97912f3be\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "GPU 0: Tesla K80 (UUID: GPU-70d7c173-7223-f7e9-7bfa-4142f8150440)\n",
            "GPU RAM 11441MB | Free 11441MB)\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWpWFeyO8APF"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_LGhTfV8APG"
      },
      "source": [
        "First, let's prepare the dataset. Ensure all your images have the same color channels (monochrome, RGB or RGBA). If you edit the images yourself (e.g. for non-square aspect ratios, or to keep the compositions), ensure their correct size.  \n",
        "For conditional model the images should be split by subfolders (mydata/1, mydata/2, ..).\n",
        "\n",
        "Upload zip-archive with images onto Google drive and type its path here (relative to G-drive root). \n",
        "\n",
        "If you work with patterns or shapes (rather than compostions), you may want to enable `multicrop` to crop square fragments from bigger images, effectively increasing amount of data (not suitable for conditional data, as it would break folder structure!). The images will be cut into `size`px fragments, overlapped with `step` shift by X and Y. If the image is smaller, it will be upscaled to the `size`. Edit these values according to your dataset.   \n",
        "*Restart session (Runtime) after `multicrop` run*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEmNzQmm0t_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffc4eed-f5da-4dab-eed2-0655389e66b1"
      },
      "source": [
        "#@title Data setup \n",
        "dataset = 'sq-512' #@param {type:\"string\"}\n",
        "source = 'sq-512.zip' #@param {type:\"string\"}\n",
        "content_data = os.path.join('/content', dataset)\n",
        "data_dir = os.path.join(work_dir, 'data/')\n",
        "#data_dir = os.path.join('/content', 'data/')\n",
        "dataset_dir = os.path.join(data_dir, dataset)\n",
        "\n",
        "#@markdown Multicrop\n",
        "multicrop = False  #@param {type:\"boolean\"}\n",
        "size = \"256\" #@param [256,512,1024]\n",
        "overlap = 128 #@param {type:\"integer\"}\n",
        "size = int(size)\n",
        "\n",
        "# cleanup previous attempts\n",
        "!rm -rf /content/tmp \n",
        "!rm -rf $content_data\n",
        "!rm -rf $dataset_dir\n",
        "\n",
        "!mkdir /content/tmp\n",
        "%cd /content/tmp\n",
        "fpath = os.path.join(gdir, source)\n",
        "!unzip -j -o -q $fpath -d $dataset\n",
        "%cd $work_dir\n",
        "\n",
        "if multicrop:\n",
        "  if os.path.isdir('/content/tmp'):\n",
        "    %run src/util/multicrop.py --in_dir /content/tmp --out_dir $content_data --size $size --step $overlap\n",
        "    !mv $content_data $data_dir\n",
        "  else:\n",
        "    print('/content/tmp not found!!')\n",
        "else:\n",
        "  !mv /content/tmp/* $data_dir\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tmp\n",
            "/G/MyDrive/sg2a_eps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfczVF0W8APH"
      },
      "source": [
        "Now, we can train StyleGAN2 on the prepared dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKmktOPb8APH"
      },
      "source": [
        "#@title Train\n",
        "%cd $work_dir\n",
        "dataset = 'sq-512' #@param {type:\"string\"}\n",
        "kimg = 5000 #@param {type:\"integer\"}\n",
        "data_dir = os.path.join(work_dir, 'data', dataset)\n",
        "\n",
        "%run src/train.py --data $data_dir --kimg $kimg --snap 1 --mirror false"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYlI1fXe8APH"
      },
      "source": [
        "> This will run training process, according to the options and settings in `src/train.py` (check and explore those!!). Results (models and samples) are saved under `train` directory, similar to original Nvidia approach. There are two types of models saved: compact (containing only Gs network for inference) as `<dataset>-...pkl` (e.g. `test-256-0360.pkl`), and full (containing G/D/Gs networks for further training) as `snapshot-...pkl`. Check sample images there to follow the progress.\n",
        "\n",
        "> The length of the training is defined by `--kimg X` argument (training duration in thousands of images). Reasonable `kimg` value for full training from scratch is 5000-8000, while for finetuning in `--d_aug` mode 1000-2000 may be sufficient.  \n",
        "\n",
        "> *NB: If you get `RuntimeError: context has already been set` (e.g. after training interruption or `multicrop` run) - restart session, run General setup and Train again.*\n",
        "\n",
        "Other training options:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAqpEFzukpfs"
      },
      "source": [
        "%run src/train.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpeUmhiH8API"
      },
      "source": [
        "If the training process was interrupted, resume it from the last saved model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr73IUPG8API",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673f7e53-d5d1-44ef-933c-d7c1757cddee"
      },
      "source": [
        "#@title Resume training\n",
        "%cd $work_dir\n",
        "dataset = 'sq-512.zip' #@param {type:\"string\"}\n",
        "resume_dir = 'train/039-sq-512-512-auto-gf_bnc' #@param {type:\"string\"}\n",
        "kimg = 1000 #@param {type:\"integer\"}\n",
        "data_dir = os.path.join(work_dir, 'data', dataset)\n",
        "#data_dir = os.path.join('/content', 'data', dataset)\n",
        "\n",
        "%run src/train.py --data $data_dir --resume $resume_dir --kimg $kimg --snap 1 --mirror false"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/G/MyDrive/sg2a_eps\n",
            " Dataset subdirs are set for labels\n",
            " Train dir:  train/040-sq-512-512-auto-gf_bnc\n",
            " Dataset:    /G/MyDrive/sg2a_eps/data/sq-512.zip\n",
            " Resolution: 512\n",
            " Batch:      8\n",
            " Length:     1000 kimg\n",
            " Augment:    gf_bnc\n",
            " Num images: 2027 \n",
            "Image shape: [3, 512, 512]\n",
            "Label shape: [0]\n",
            "Constructing networks...\n",
            " Resuming from \"train/039-sq-512-512-auto-gf_bnc/snapshot-512-0024.pkl\", kimg 24\n",
            "Training for 1000 kimg...\n",
            "\n",
            "tick 0    kimg 24.0   time 1m 57s    136d 19:00:04s left till 22:44  min/tick 1.61   sec/kimg 1.21e+04 cpumem 2.6  gpumem 8.7  aug 0.000 lr 0.0025 0.0025\n",
            "tick 1    kimg 28.0   time 1h 10m 01s  11d 11:12:06s left till 16:04  min/tick 68     sec/kimg 1.02e+03 cpumem 3.3  gpumem 8.1  aug 0.034 lr 0.0025 0.0025\n",
            "tick 2    kimg 32.0   time 2h 18m 02s  11d 10:01:58s left till 16:02  min/tick 67.9   sec/kimg 1.02e+03 cpumem 3.3  gpumem 8.1  aug 0.070 lr 0.0025 0.0025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVxyOjz48API"
      },
      "source": [
        "NB: In most cases it's much easier to use a \"transfer learning\" trick, rather than perform full training from the scratch. For that, we use existing well-trained model as a starter, and \"finetune\" (uptrain) it with our data. This works pretty well, even if our dataset is very different from the original model. \n",
        "\n",
        "So here is a faster way to train our GAN (presuming we have full trained model `train/ffhq-256.pkl` already):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3kwDOh88APJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d742e7b-3f32-40ac-d211-dce272ef9006"
      },
      "source": [
        "#@title Finetune training\n",
        "%cd $work_dir\n",
        "dataset = 'sq-512.zip' #@param {type:\"string\"}\n",
        "resume_pkl = 'train/2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664.pkl' #@param {type:\"string\"}\n",
        "kimg = 1000 #@param {type:\"integer\"}\n",
        "data_dir = os.path.join(work_dir, 'data', dataset)\n",
        "\n",
        "%run src/train.py --data $data_dir --resume $resume_pkl --kimg $kimg --snap 1 --mirror false"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/G/MyDrive/sg2a_eps\n",
            " Dataset subdirs are set for labels\n",
            " Train dir:  train/033-sq-512-512-auto-gf_bnc\n",
            " Dataset:    /G/MyDrive/sg2a_eps/data/sq-512.zip\n",
            " Resolution: 512\n",
            " Batch:      8\n",
            " Length:     1000 kimg\n",
            " Augment:    gf_bnc\n",
            " Num images: 2027 \n",
            "Image shape: [3, 512, 512]\n",
            "Label shape: [0]\n",
            "Constructing networks...\n",
            " Resuming from \"train/2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664.pkl\", kimg 0\n",
            "Training for 1000 kimg...\n",
            "\n",
            "tick 0    kimg 0.0    time 2m 02s    144d 18:42:56s left till 02:30  min/tick 1.67   sec/kimg 1.25e+04 cpumem 2.6  gpumem 8.7  aug 0.000 lr 0.0025 0.0025\n",
            "tick 1    kimg 4.0    time 1h 13m 31s  12d 8:08:21s left till 17:07  min/tick 71.4   sec/kimg 1.07e+03 cpumem 3.3  gpumem 8.1  aug 0.034 lr 0.0025 0.0025\n",
            "tick 2    kimg 8.0    time 2h 25m 02s  12d 7:18:04s left till 17:28  min/tick 71.4   sec/kimg 1.07e+03 cpumem 3.3  gpumem 8.1  aug 0.068 lr 0.0025 0.0025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Njelbgu8APJ"
      },
      "source": [
        "## Generation\n",
        "\n",
        "Let's produce some imagery from the original cat model (get it [here](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl) and put onto Google drive within our working directory).  \n",
        "Generated results are saved as sequences and videos (by default, under `_out` directory).  \n",
        "More cool models can be found [here](https://github.com/justinpinkney/awesome-pretrained-stylegan2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S73oknl8wLU"
      },
      "source": [
        "#@title ### Generator setup\n",
        "!cd $work_dir\n",
        "\n",
        "output = '_out/cats' #@param {type:\"string\"}\n",
        "\n",
        "frames = 50 #@param {type:\"integer\"}\n",
        "frame_step = 10 #@param {type:\"integer\"}\n",
        "timeframe = '%d-%d' % (frames, frame_step)\n",
        "\n",
        "cubic_smooth = True #@param {type:\"boolean\"}\n",
        "gauss_smooth = False #@param {type:\"boolean\"}\n",
        "cubic_smooth = '--cubic ' if cubic_smooth else ''\n",
        "gauss_smooth = '--gauss ' if gauss_smooth else ''\n",
        "smooth = cubic_smooth + gauss_smooth\n",
        "\n",
        "save_lat = False #@param {type:\"boolean\"}\n",
        "save_lat = '--save_lat ' if save_lat else ''\n",
        "\n",
        "seed = 0 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Select model from the dropdown below:\n",
        "model_select(work_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlzqD3DEGzIH"
      },
      "source": [
        "> This means loading the model, and producing 50 frames, interpolating between random latent (`z`) space keypoints, with a step of 10 frames between keypoints. \n",
        "`save_lat` option = save all traversed dlatent (`w`) points as Numpy array in `*.npy` file (useful for further curating). `cubic` option changes linear interpolation to cubic for smoother animation; `gauss` provides additional smoothing. Set `seed` value to produce repeatable results (0 = random)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBHD0n0i8APJ"
      },
      "source": [
        "#@title ### Native generation\n",
        "%cd $work_dir\n",
        "%run src/_genSGAN2.py --model $model_pkl --out_dir $output --frames $timeframe $smooth $save_lat --seed $seed\n",
        "ipython_display(ImageSequenceClip(img_list(output), fps=25), center=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwoOBOcR8APK",
        "cellView": "form"
      },
      "source": [
        "#@title ### Custom size generation\n",
        "\n",
        "sizeX = 400 #@param {type:\"integer\"} \n",
        "sizeY = 300 #@param {type:\"integer\"}\n",
        "size = '%d-%d' % (sizeX, sizeY)\n",
        "scaling = 'pad' #@param ['pad', 'padside', 'symm', 'symmside']\n",
        "\n",
        "%cd $work_dir\n",
        "%run src/_genSGAN2.py --model $model --out_dir $output --frames $timeframe --size $size --scale_type $scaling  $smooth $save_lat --seed $seed\n",
        "ipython_display(ImageSequenceClip(img_list(output), fps=25), center=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOGVIJt18APL",
        "cellView": "form"
      },
      "source": [
        "#@title ### Multi-latent  generation\n",
        "\n",
        "sizeX = 768 #@param {type:\"integer\"} \n",
        "sizeY = 256 #@param {type:\"integer\"}\n",
        "size = '%d-%d' % (sizeX, sizeY)\n",
        "scaling = 'pad' #@param ['pad', 'padside', 'symm', 'symmside']\n",
        "\n",
        "split_X =  3#@param {type:\"integer\"} \n",
        "split_Y =  1#@param {type:\"integer\"}\n",
        "split = '%d-%d' % (split_X, split_Y)\n",
        "splitfine = 0. #@param {type:\"number\"}\n",
        "\n",
        "%cd $work_dir\n",
        "%run src/_genSGAN2.py --model $model --out_dir $output --frames $timeframe --size $size --scale_type $scaling -n $split --splitfine $splitfine $smooth --seed $seed\n",
        "ipython_display(ImageSequenceClip(img_list(output), fps=25), center=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ3zGyFy8APL"
      },
      "source": [
        "> Here we get animated composition of 3 independent frames, blended together horizontally.  \n",
        "`splitfine` controls boundary fineness (0 = smoothest/default, higher => thinner).  \n",
        "\n",
        "Instead of frame splitting, we can load external mask from b/w image file (or folder with image sequence):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB0-VZ798APL",
        "cellView": "form"
      },
      "source": [
        "#@title ### Masked  generation\n",
        "\n",
        "sizeX = 400 #@param {type:\"integer\"} \n",
        "sizeY = 300 #@param {type:\"integer\"}\n",
        "size = '%d-%d' % (sizeX, sizeY)\n",
        "\n",
        "lat_mask = '_in/mask.jpg' #@param {type:\"string\"} \n",
        "\n",
        "%cd $work_dir\n",
        "%run src/_genSGAN2.py --model $model --out_dir $output --frames $timeframe --size $size --latmask $lat_mask  $smooth --seed $seed\n",
        "ipython_display(ImageSequenceClip(img_list(output), fps=25), center=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWUJDcGW8APM"
      },
      "source": [
        "`digress` adds some funky displacements by tweaking initial constant layer.  \n",
        "`truncation` controls variety, as usual  (0 = boring, >1 = weird). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tITi-ILU8APM",
        "cellView": "form"
      },
      "source": [
        "#@title ### Other tricks\n",
        "\n",
        "sizeX = 400 #@param {type:\"integer\"} \n",
        "sizeY = 300 #@param {type:\"integer\"}\n",
        "size = '%d-%d' % (sizeX, sizeY)\n",
        "\n",
        "digress =  2#@param {type:\"number\"} \n",
        "truncation =  1.5#@param {type:\"number\"} \n",
        "\n",
        "%cd $work_dir\n",
        "%run src/_genSGAN2.py --model $model --out_dir $output --frames $timeframe --size $size --digress $digress --trunc $truncation $smooth --seed $seed\n",
        "ipython_display(ImageSequenceClip(img_list(output), fps=25), center=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLBZAOSi8APM"
      },
      "source": [
        "## Latent space exploration\n",
        "\n",
        "For these experiments download [FFHQ model](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-f.pkl) and save to `models` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB7ujwkD8APM",
        "cellView": "form"
      },
      "source": [
        "#@title ### Generator setup\n",
        "!cd $work_dir\n",
        "\n",
        "output = '_out/ffhq' #@param {type:\"string\"}\n",
        "\n",
        "frames = 50 #@param {type:\"integer\"}\n",
        "frame_step = 10 #@param {type:\"integer\"}\n",
        "timeframe = '%d-%d' % (frames, frame_step)\n",
        "\n",
        "cubic_smooth = True #@param {type:\"boolean\"}\n",
        "gauss_smooth = False #@param {type:\"boolean\"}\n",
        "cubic_smooth = '--cubic ' if cubic_smooth else ''\n",
        "gauss_smooth = '--gauss ' if gauss_smooth else ''\n",
        "smooth = cubic_smooth + gauss_smooth\n",
        "\n",
        "save_lat = False #@param {type:\"boolean\"}\n",
        "save_lat = '--save_lat ' if save_lat else ''\n",
        "\n",
        "#@markdown Select model from the dropdown list below:\n",
        "model_select(work_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIbPEUyI8APN"
      },
      "source": [
        "Project external images onto the model dlatent `w` space, saving points as Numpy arrays in `*.npy` files. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJx9ws7n8APN",
        "cellView": "form"
      },
      "source": [
        "#@title ### Images projection\n",
        "\n",
        "image_dir = '_in/photo' #@param {type:\"string\"} \n",
        "out_dir = '_out/proj' #@param {type:\"string\"} \n",
        "steps =  1000#@param {type:\"integer\"} \n",
        "\n",
        "%cd $work_dir\n",
        "%run src/projector.py --model $model_pkl --in_dir _in/photo --out_dir _out/proj --steps $steps\n",
        "\n",
        "from src.util.utilgan import img_list\n",
        "images = img_list(out_dir, subdir=True)\n",
        "images = [f for f in images if 'target.jpg' not in f]\n",
        "Image(images[0], width=512, height=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6l471qJWsX2"
      },
      "source": [
        "Produce looped animation, interpolating between saved dlatent points (from a file or directory with `*.npy` or `*.npz` files). To select only few frames from a sequence `xxx.npy`, create text file with comma-delimited frame numbers and save it as `xxx.txt` in the same directory (check examples in `_in/dlats`). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIJJUfAN8APN",
        "cellView": "form"
      },
      "source": [
        "#@title ### \"Walk\" between saved dlatent points\n",
        "\n",
        "dlatents = '_in/dlats' #@param {type:\"string\"} \n",
        "out_dir = '_out/ffhq-dlats' #@param {type:\"string\"} \n",
        "\n",
        "%cd $work_dir\n",
        "%run src/_play_dlatents.py --model $model --dlatents $dlatents --out_dir $out_dir --fstep $frame_step $smooth\n",
        "ipython_display(ImageSequenceClip(img_list(out_dir), fps=25), center=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NBBc2CV8APN"
      },
      "source": [
        "\n",
        "\n",
        "We can also load another dlatent point and apply it to higher network layers, effectively \"stylizing\" images with its' fine features.  \n",
        "`digress` and `truncation` are also applicable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J84NdtP38APO",
        "cellView": "form"
      },
      "source": [
        "#@title ### \"Walk\" between saved points w/tricks\n",
        "\n",
        "dlatents = '_in/dlats' #@param {type:\"string\"} \n",
        "out_dir = '_out/ffhq-dlats' #@param {type:\"string\"} \n",
        "style_dlat = '_in/blonde458.npy' #@param {type:\"string\"} \n",
        "digress =  2#@param {type:\"number\"} \n",
        "truncation =  1.5#@param {type:\"number\"} \n",
        "\n",
        "%cd $work_dir\n",
        "%run src/_play_dlatents.py --model $model --dlatents $dlatents --out_dir $out_dir --fstep $frame_step $smooth --style_dlat $style_dlat --digress $digress --trunc $truncation\n",
        "ipython_display(ImageSequenceClip(img_list(out_dir), fps=25), center=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iykr7uKL8APO"
      },
      "source": [
        "Generate animation by walking from saved dlatent point along feature direction vectors (aging/smiling/etc) one by one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOToqrX68APO",
        "scrolled": true,
        "cellView": "form"
      },
      "source": [
        "#@title ### \"Walk\" along feature directions\n",
        "\n",
        "base_lat = '_in/blonde458.npy' #@param {type:\"string\"} \n",
        "vectors = '_in/vectors_ffhq' #@param {type:\"string\"} \n",
        "out_dir = '_out/ffhq_looks' #@param {type:\"string\"} \n",
        "\n",
        "%cd $work_dir\n",
        "%run src/_play_vectors.py --model $model_pkl --base_lat $base_lat --vector_dir $vectors --out_dir $out_dir --fstep $frame_step\n",
        "ipython_display(ImageSequenceClip(img_list(out_dir), fps=25), center=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN_82CUzLWdL"
      },
      "source": [
        "> Try discovering more such vectors:\n",
        "> * https://github.com/genforce/sefa\n",
        "> * https://github.com/harskish/ganspace"
      ]
    }
  ]
}